深度学习之PyTorch实战_在线课程学习_万门大学

全网it学习资源共享学习<br>全网课程都有，欢迎私聊<br>微信：jerryttom<br>

┣━wm012-深度学习之PyTorch实战_在线课程学习_万门-大学<br> ┣━7.3自定义层.mp4<br> ┣━9.6残差网络（ResNet）新 .mp4<br> ┣━10.4实战：循环神经网络的从零实现.mp4<br> ┣━2.3PyTorch自动求梯度.mp4<br> ┣━6.3正反向传播、计算图与数值稳定性、模型初始化 .mp4<br> ┣━9.3网络中的网络（NiN）新 .mp4<br> ┣━12.1优化与深度学习 .mp4<br> ┣━1.1历史背景 .mp4<br> ┣━6.2权重衰减与丢弃法.mp4<br> ┣━10.5实战：循环神经网络的简洁实现 .mp4<br> ┣━8.2多通道与池化层 .mp4<br> ┣━5.2实战：多层感知机的从零实现 .mp4<br> ┣━9.7稠密连接网络（DenseNet）新 .mp4<br> ┣━5.1多层感知机概述.mp4<br> ┣━7.1模型构造.mp4<br> ┣━8.4实战：卷积神经网络（一）.mp4<br> ┣━11.2门控循环单元（GRU）.mp4<br> ┣━2.2PyTorch基本数据操作 .mp4<br> ┣━8.1卷积、填充与步幅 .mp4<br> ┣━12.2梯度下降和随机梯度下降.mp4<br> ┣━9.2使用块的网络 (VGG)新 .mp4<br> ┣━4.4实战：softmax的简洁实现.mp4<br> ┣━7.4读取与存储 .mp4<br> ┣━9.1深度卷积神经网络 (AlexNet)新.mp4<br> ┣━1.2适用场景和学习方法.mp4<br> ┣━9.5批量归一化新.mp4<br> ┣━课程资料<br> ┣━第7讲：PyTorch构建模型 .zip<br> ┣━第8讲：卷积神经网络（一） .zip<br> ┣━第1讲：深度学习简介 .pdf<br> ┣━第13讲：优化算法（二） .zip<br> ┣━第12讲：优化算法（一） .zip<br> ┣━第4讲：深度学习基础：softmax回归 .zip<br> ┣━第10讲：循环神经网络（一） .zip<br> ┣━第5讲：深度学习基础：多层感知机 .zip<br> ┣━第6讲：神经网络模型与运算 .zip<br> ┣━第3讲：深度学习基础：线性回归 .zip<br> ┣━第2讲：PyTorch预备知识 .zip<br> ┣━第9讲：卷积神经网络（二） .zip<br> ┣━补充代码文件：dl4wm .zip<br> ┣━第11讲：循环神经网络（二） .zip<br> ┣━6.1模型选择、欠拟合和过拟合 .mp4<br> ┣━8.5实战：卷积神经网络（二）.mp4<br> ┣━13.1动量法 .mp4<br> ┣━8.3卷积神经网络 .mp4<br> ┣━13.3实战：RMProp算法.mp4<br> ┣━5.3实战：多层感知机的简洁实现.mp4<br> ┣━12.4实战：小批量随机梯度下降.mp4<br> ┣━3.2实战：线性回归代码.mp4<br> ┣━7.5GPU计算.mp4<br> ┣━12.3实战：梯度下降和随机梯度下降 .mp4<br> ┣━6.4实战：Kaggle比赛：房价预测.mp4<br> ┣━11.4实战：深层循环神经网络.mp4<br> ┣━2.1获取代码和安装运行环境.mp4<br> ┣━4.1softmax回归概述 .mp4<br> ┣━13.4实战：AdaDelta算法 .mp4<br> ┣━2.4如何查阅文档和寻求帮助 .mp4<br> ┣━13.5实战：Adam算法.mp4<br> ┣━3.3实战：线性回归的简洁实现.mp4<br> ┣━9.4含并行连结的网络（GoogLeNet）新 .mp4<br> ┣━13.2实战：AdaGrad算法 .mp4<br> ┣━11.1通过时间反向传播.mp4<br> ┣━3.1线性回归 .mp4<br> ┣━10.3实战：语言模型数据集 .mp4<br> ┣━10.2循环神经网络 .mp4<br> ┣━4.2实战：图像分类数据集（Fashion-MNIST）.mp4<br> ┣━10.1语言模型.mp4<br> ┣━7.2模型参数的访问、初始化和共享.mp4<br> ┣━11.3长短期记忆网络（LSTM） .mp4<br> ┣━4.3实战：softmax回归从零实现.mp4